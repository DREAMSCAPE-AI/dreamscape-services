# DREAMSCAPE ML Training Dataset Pipeline

Pipeline ETL pour crÃ©er le dataset d'entraÃ®nement du modÃ¨le de recommandation ML.

## ğŸ“‹ Vue d'ensemble

Ce pipeline extrait les donnÃ©es de PostgreSQL (UserVector, Recommendations, ItemVector, etc.), les transforme en features ML, et gÃ©nÃ¨re un dataset prÃªt Ã  l'entraÃ®nement au format Parquet.

**Sorties** :
- `train_v1.0.parquet` - Dataset d'entraÃ®nement (80%)
- `test_v1.0.parquet` - Dataset de test (20%)
- `metadata_v1.0.json` - MÃ©tadonnÃ©es du dataset
- `quality_report_v1.0.html` - Rapport qualitÃ© avec graphiques

## ğŸ¯ Features du Dataset

### User Features (8D vector)
- `user_climate_pref`, `user_culture_pref`, `user_budget_level`, `user_activity_level`
- `user_group_type`, `user_urban_pref`, `user_gastronomy_pref`, `user_popularity_pref`

### User Metadata
- `primary_segment` (BUDGET_BACKPACKER, FAMILY_EXPLORER, LUXURY_TRAVELER, etc.)
- `user_age_group`, `user_region`, `user_category`

### Item Features (8D vector)
- `item_climate`, `item_culture`, `item_budget`, `item_activity`, etc.
- `item_popularity_score`, `item_booking_count`, `item_search_count`

### Context Features
- `season`, `is_weekend`, `days_until_departure`, `search_passengers`

### Labels
- `engagement_score` : 0 (not_viewed), 1 (viewed), 3 (clicked), 5 (booked), -1 (rejected)
- `booking_probability` : 1 (booked), 0 (not_booked)

## ğŸš€ Utilisation

### Installation locale

```bash
cd dreamscape-services/ai

# CrÃ©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Installer dÃ©pendances
pip install -r ml/requirements.txt

# Configurer DATABASE_URL
export DATABASE_URL="postgresql://dreamscape:password@localhost:5432/dreamscape"

# Lancer le pipeline
bash ml/scripts/run_etl.sh 1.0 90

# Ou directement en Python
python ml/scripts/run_etl.py --version 1.0 --window-days 90
```

### Docker

```bash
cd dreamscape-infra

# Build l'image ML
docker-compose build ai-ml-trainer

# Lancer le pipeline ETL
docker-compose run --rm ai-ml-trainer

# Avec paramÃ¨tres personnalisÃ©s
docker-compose run --rm ai-ml-trainer python ml/scripts/run_etl.py --version 1.1 --window-days 180

# VÃ©rifier les outputs
docker-compose run --rm ai-ml-trainer ls -lh /app/data/datasets/v1.0/
```

## ğŸ“Š Pipeline ETL (10 Ã©tapes)

1. **Extract User Data** - Extraction User + UserVector + TravelOnboardingProfile
2. **Extract Recommendations** - Extraction Recommendation + ItemVector
3. **Extract Voyage Data** - Extraction SearchHistory + BookingData
4. **Merge Datasets** - Fusion des 3 sources
5. **Feature Engineering** - Unpacking 8D vectors, calculs temporels
6. **Label Construction** - engagement_score, booking_probability
7. **Negative Sampling** - Ajout Ã©chantillons nÃ©gatifs (ratio 2:1)
8. **Data Cleaning** - Outliers, missing values, duplicates
9. **GDPR Anonymization** - Hash userId, suppression PII
10. **Export** - Train/test split, Parquet, metadata, rapport

## ğŸ“ Structure des Fichiers

```
ml/
â”œâ”€â”€ etl/                          # Scripts ETL
â”‚   â”œâ”€â”€ extract_user_data.py      # Extraction users
â”‚   â”œâ”€â”€ extract_recommendations.py # Extraction recommendations
â”‚   â”œâ”€â”€ extract_voyage_data.py    # Extraction searches
â”‚   â”œâ”€â”€ merge_datasets.py         # Fusion datasets
â”‚   â”œâ”€â”€ feature_engineering.py    # Feature engineering
â”‚   â”œâ”€â”€ label_construction.py     # Construction labels
â”‚   â”œâ”€â”€ negative_sampling.py      # Negative sampling
â”‚   â”œâ”€â”€ data_cleaning.py          # Nettoyage
â”‚   â”œâ”€â”€ gdpr_anonymization.py     # Anonymisation
â”‚   â””â”€â”€ export_dataset.py         # Export final
â”‚
â”œâ”€â”€ config/                       # Configuration
â”‚   â”œâ”€â”€ dataset_config.py         # SchÃ©ma dataset
â”‚   â”œâ”€â”€ db_config.py              # Configuration DB
â”‚   â””â”€â”€ logging_config.py         # Configuration logs
â”‚
â”œâ”€â”€ utils/                        # Utilitaires
â”‚   â”œâ”€â”€ logger.py                 # Logger
â”‚   â”œâ”€â”€ db_connector.py           # Connexion PostgreSQL
â”‚   â”œâ”€â”€ validators.py             # Validation dataset
â”‚   â””â”€â”€ metrics.py                # Calcul mÃ©triques
â”‚
â”œâ”€â”€ scripts/                      # Scripts d'exÃ©cution
â”‚   â”œâ”€â”€ run_etl.py                # Orchestrateur Python
â”‚   â””â”€â”€ run_etl.sh                # Script shell
â”‚
â””â”€â”€ requirements.txt              # DÃ©pendances Python
```

## âš™ï¸ Configuration

### Variables d'environnement

```bash
DATABASE_URL=postgresql://dreamscape:password@postgres:5432/dreamscape
DATASET_VERSION=1.0
DATA_WINDOW_DAYS=90
NEGATIVE_SAMPLE_RATIO=2.0
TEST_SIZE=0.2
RANDOM_SEED=42
LOG_LEVEL=INFO
LOG_FILE=logs/etl.log
```

### Fichier de configuration

Voir [config/dataset_config.py](config/dataset_config.py) pour :
- SchÃ©ma complet (52+ colonnes)
- Mapping des colonnes
- Constantes (window, ratio, seed)

## âœ… Validation du Dataset

AprÃ¨s gÃ©nÃ©ration, vÃ©rifier :

```bash
# VÃ©rifier les fichiers
ls -lh data/datasets/v1.0/

# VÃ©rifier le contenu (Python)
python -c "
import pandas as pd
df = pd.read_parquet('data/datasets/v1.0/train_v1.0.parquet')
print(f'Rows: {len(df):,}')
print(f'Columns: {len(df.columns)}')
print(f'Booking rate: {df["booking_probability"].mean():.2%}')
print(f'Engagement distribution:\n{df["engagement_score"].value_counts()}')
"

# Ouvrir rapport qualitÃ©
open data/datasets/v1.0/quality_report_v1.0.html  # macOS
# start data/datasets/v1.0/quality_report_v1.0.html  # Windows
```

VÃ©rifications critiques :
- âœ… `train_v1.0.parquet` > 10 MB
- âœ… `test_v1.0.parquet` > 2 MB
- âœ… Booking rate entre 5-15%
- âœ… Engagement score balanced
- âœ… Pas de valeurs manquantes dans colonnes requises
- âœ… Vecteurs dans [0, 1]

## ğŸ”„ Versioning du Dataset

### Convention de nommage

- **v1.0** - Version initiale (90 jours)
- **v1.1** - Refresh hebdomadaire (data refresh)
- **v2.0** - Changement majeur (schema, window)

### CrÃ©er nouvelle version

```bash
# Refresh hebdomadaire
bash ml/scripts/run_etl.sh 1.1 90

# Changement de window
bash ml/scripts/run_etl.sh 2.0 180
```

## ğŸ“ˆ MÃ©triques du Dataset

Le rapport qualitÃ© (`quality_report_v1.0.html`) inclut :
- Distribution engagement_score
- Distribution user budget
- Distribution item popularity
- Distribution segments
- Taux de booking
- Missing values par colonne
- Statistiques train/test

## ğŸ› DÃ©pannage

### Erreur de connexion DB

```bash
# VÃ©rifier que PostgreSQL est accessible
psql $DATABASE_URL -c "SELECT 1"

# VÃ©rifier la configuration
echo $DATABASE_URL
```

### Pas assez de donnÃ©es

```bash
# VÃ©rifier les donnÃ©es sources
psql $DATABASE_URL -c "SELECT COUNT(*) FROM recommendations WHERE \"createdAt\" >= NOW() - INTERVAL '90 days'"
```

### Erreur de mÃ©moire

```python
# RÃ©duire la fenÃªtre de donnÃ©es
python ml/scripts/run_etl.py --window-days 30
```

## ğŸ“š Documentation

- [Plan d'implÃ©mentation](../../.claude/plans/) - Plan dÃ©taillÃ© du pipeline
- [Schema dataset](config/dataset_config.py) - SchÃ©ma complet
- [Documentation Prisma](../../db/prisma/schema.prisma) - SchÃ©ma base de donnÃ©es

## ğŸ”— Prochaines Ã©tapes

AprÃ¨s gÃ©nÃ©ration du dataset :

1. **US-IA-008** - EntraÃ®nement du modÃ¨le ML (Collaborative Filtering)
2. Export embeddings vers PostgreSQL
3. IntÃ©gration avec serveur TypeScript pour serving
4. A/B testing ML vs rÃ¨gles actuelles

## ğŸ“ Livrables (US-IA-007)

âœ… Script ETL : `ml/etl/*.py` (10 scripts)
âœ… Dataset v1.0 : Format Parquet (train + test)
âœ… Documentation schema : `config/dataset_config.py`
âœ… Rapport qualitÃ© : `quality_report_v1.0.html`

**Story Points** : 8
